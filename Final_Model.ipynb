{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# for ensemble method use\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectKBest,f_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "#TO DO-\n",
    "#One-R\n",
    "# sampling SOMTE\n",
    "\n",
    "\n",
    "# height removed\n",
    "Test_Target = [\"SUSPECT_ARRESTED_FLAG\", \"STOP_WAS_INITIATED\",\n",
    "                 \"OBSERVED_DURATION_MINUTES\", \"SUSPECTED_CRIME_DESCRIPTION\",\n",
    "                 \"STOP_DURATION_MINUTES\", \"FIREARM_FLAG\",\n",
    "                 \"SUSPECT_RACE_DESCRIPTION\",\n",
    "                 \"DEMEANOR_OF_PERSON_STOPPED\", \"SUSPECT_BODY_BUILD_TYPE\",\"FRISKED_FLAG\",\"SEARCHED_FLAG\",\"STOP_LOCATION_BORO_NAME\"]\n",
    "Test1 = pd.read_excel(\"sqf2022.xlsx\", usecols=Test_Target)\n",
    "\n",
    "Feature_test=[\"FIREARM_FLAG\",\"FRISKED_FLAG\",\"SEARCHED_FLAG\"]\n",
    "\n",
    "Test1['FIREARM_FLAG'] = Test1['FIREARM_FLAG'].replace('(null)', 'No')\n",
    "Test1.replace('(null)', np.nan, inplace=True)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "Test1[\"SUSPECT_ARRESTED_FLAG\"] = lb.fit_transform(Test1[\"SUSPECT_ARRESTED_FLAG\"])\n",
    "Test1[\"FIREARM_FLAG\"] = lb.fit_transform(Test1[\"FIREARM_FLAG\"])\n",
    "Test1[\"FRISKED_FLAG\"] = lb.fit_transform(Test1[\"FRISKED_FLAG\"])\n",
    "Test1[\"SEARCHED_FLAG\"] = lb.fit_transform(Test1[\"SEARCHED_FLAG\"])\n",
    "\n",
    "\n",
    "Test1[\"SUSPECT_ARRESTED_FLAG\"].fillna(Test1[\"SUSPECT_ARRESTED_FLAG\"].median,inplace=True)\n",
    "Test1[\"FIREARM_FLAG\"].fillna(Test1[\"FIREARM_FLAG\"].median,inplace=True)\n",
    "Test1[\"FRISKED_FLAG\"].fillna(Test1[\"FRISKED_FLAG\"].median,inplace=True)\n",
    "Test1[\"SEARCHED_FLAG\"].fillna(Test1[\"SEARCHED_FLAG\"].median,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot-Encoding\n",
    "OneHot_Encode_feature = [\"STOP_WAS_INITIATED\",\"SUSPECTED_CRIME_DESCRIPTION\"]\n",
    "Label_En = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "Feature_Encoded = Label_En.fit_transform(Test1[OneHot_Encode_feature])\n",
    "Label_df = pd.DataFrame(Feature_Encoded.toarray(), columns=Label_En.get_feature_names_out(OneHot_Encode_feature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding\n",
    "LE=LabelEncoder()\n",
    "Test1['SUSPECT_BODY_BUILD_TYPE']=LE.fit_transform(Test1[\"SUSPECT_BODY_BUILD_TYPE\"])\n",
    "Test1['SUSPECT_BODY_BUILD_TYPE'].fillna(Test1['SUSPECT_BODY_BUILD_TYPE'].mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import numpy as np\n",
    "\n",
    "# # Assuming you have a list of texts in the 'DEMEANOR' feature\n",
    "# texts = Test1['DEMEANOR_OF_PERSON_STOPPED'].fillna('').tolist()\n",
    "\n",
    "# # Initialize the CountVectorizer\n",
    "# vectorizer = CountVectorizer()\n",
    "\n",
    "# # Fit and transform the texts using the CountVectorizer\n",
    "# X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# # Get the feature names\n",
    "# feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# # Convert the vectorized data to a DataFrame\n",
    "# vectorized_data = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "\n",
    "# # Print the vectorized data\n",
    "# print(vectorized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15102\n",
      "15102\n",
      "15102\n",
      "Number of null values in 'column_name': 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (15102, 1), indices imply (15102, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l0/dqqn7htx52gbmyf1bnbpby0m0000gn/T/ipykernel_2440/1507720142.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Create DataFrame from list of tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mfinal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DEMEANOR_OF_PERSON_STOPPED'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'COMPOUND_SENTIMENT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    735\u001b[0m                     )\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m                     mgr = ndarray_to_mgr(\n\u001b[0m\u001b[1;32m    738\u001b[0m                         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m                         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    349\u001b[0m     )\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m     \u001b[0m_check_values_indices_shape_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_check_values_indices_shape_match\u001b[0;34m(values, index, columns)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mpassed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mimplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (15102, 1), indices imply (15102, 2)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nlpp = pd.read_excel(\"sqf2022.xlsx\", usecols=[\"DEMEANOR_OF_PERSON_STOPPED\"])\n",
    "print(len(nlpp))\n",
    "\n",
    "nlp = nlpp.fillna(value='neutral')\n",
    "print(len(nlp))\n",
    "\n",
    "null_count = nlp['DEMEANOR_OF_PERSON_STOPPED'].isnull().sum()\n",
    "print(f\"Number of null values in 'DEMEANOR_OF_PERSON_STOPPED': {null_count}\")\n",
    "\n",
    "dataa = nlp['DEMEANOR_OF_PERSON_STOPPED'].tolist()\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "new_data = [analyzer.polarity_scores(element)['compound'] for element in dataa]\n",
    "\n",
    "final_df = pd.DataFrame({'DEMEANOR_OF_PERSON_STOPPED': dataa, 'COMPOUND_SENTIMENT': new_data})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['STOP_WAS_INITIATED' 'SUSPECTED_CRIME_DESCRIPTION'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l0/dqqn7htx52gbmyf1bnbpby0m0000gn/T/ipykernel_2218/740250254.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# drop the original categorical feature column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mTest1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTest1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOneHot_Encode_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mTest1\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mTest1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"DEMEANOR_OF_PERSON_STOPPED\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4955\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4956\u001b[0m         \"\"\"\n\u001b[0;32m-> 4957\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   4958\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4959\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4265\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4266\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4267\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[1;32m   4338\u001b[0m                 \u001b[0mlabels_missing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4339\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raise\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlabels_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4340\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4342\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['STOP_WAS_INITIATED' 'SUSPECTED_CRIME_DESCRIPTION'] not found in axis\""
     ]
    }
   ],
   "source": [
    "\n",
    "Test1 = pd.concat([Test1, Label_df], axis=1)\n",
    "#Test1=pd.concat([Test1,vectorized_data],axis=1)\n",
    "Test1 = pd.concat([Test1, final_df], axis=1)\n",
    "\n",
    "#cat_feature=list(Label_df.columns)+Feature_test+list(vectorized_data.columns)\n",
    "cat_feature=list(Label_df.columns)+Feature_test+[\"COMPOUND_SENTIMENT\"]\n",
    "\n",
    "# drop the original categorical feature column\n",
    "# Test1 = Test1.drop(columns=OneHot_Encode_feature)\n",
    "# Test1= Test1.drop(columns=\"DEMEANOR_OF_PERSON_STOPPED\")\n",
    "\n",
    "Test1.to_csv('output.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data,test_data=train_test_split(Test1,test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "#classi\n",
    "X_train = train_data[cat_feature] # training features_for Random forest\n",
    "y_train = train_data['SUSPECT_ARRESTED_FLAG'] # training target variable for random forest\n",
    "\n",
    "X_test = test_data[cat_feature] # testing features\n",
    "y_test = test_data['SUSPECT_ARRESTED_FLAG'] # testing target variable\n",
    "\n",
    "\n",
    "smote = SMOTE()\n",
    "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Accuracy: 0.8311817279046674\n",
      "Accuracy: 0.8342024666832216\n"
     ]
    }
   ],
   "source": [
    "Model_N=MultinomialNB()\n",
    "Model_N.fit(X_train,y_train)\n",
    "N_prediction=Model_N.predict(X_test)\n",
    "train_N_prediction=Model_N.predict(X_train)\n",
    "N_train_accuracy=accuracy_score(y_train,train_N_prediction)\n",
    "N_accuracy=accuracy_score(y_test,N_prediction)\n",
    "\n",
    "print('Train_Accuracy:', N_accuracy)\n",
    "print('Accuracy:', N_train_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Accuracy: 0.8113464635203765\n",
      "Accuracy: 0.7838464084740152\n"
     ]
    }
   ],
   "source": [
    "# Model_N_Over = MultinomialNB()\n",
    "# Model_N_Over.fit(X_train_oversampled, y_train_oversampled)\n",
    "# N_prediction_Over = Model_N_Over.predict(X_test)\n",
    "# train_N_prediction_Over = Model_N_Over.predict(X_train_oversampled)\n",
    "# N_train_accuracy_Over = accuracy_score(y_train_oversampled, train_N_prediction_Over)\n",
    "# N_accuracy_Over = accuracy_score(y_test, N_prediction_Over)\n",
    "\n",
    "# print('Train_Accuracy:', N_train_accuracy_Over)\n",
    "# print('Accuracy:', N_accuracy_Over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Accuracy: 0.9125072427779157\n",
      "Accuracy: 0.8563389606090699\n"
     ]
    }
   ],
   "source": [
    "model_R = RandomForestClassifier(random_state=42)\n",
    "#model_R.fit(train_data[Feature_test], train_data[\"SUSPECT_ARRESTED_FLAG\"])\n",
    "\n",
    "#with only caterogical data but transform to numeric/ with both\n",
    "\n",
    "model_R.fit(X_train,y_train)\n",
    "Prediction=model_R.predict(X_test)\n",
    "\n",
    "train_Prediction =model_R.predict(X_train)\n",
    "\n",
    "\n",
    "train_accuracy=accuracy_score(y_train,train_Prediction)\n",
    "\n",
    "accuracy = accuracy_score(y_test, Prediction)\n",
    "\n",
    "\n",
    "print('Train_Accuracy:', train_accuracy)\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Accuracy: 0.892914653784219\n",
      "Accuracy: 0.8331678252234359\n"
     ]
    }
   ],
   "source": [
    "model_R_Over = RandomForestClassifier(random_state=42)\n",
    "#model_R.fit(train_data[Feature_test], train_data[\"SUSPECT_ARRESTED_FLAG\"])\n",
    "\n",
    "#with only caterogical data but transform to numeric/ with both\n",
    "\n",
    "model_R_Over.fit(X_train_oversampled,y_train_oversampled)\n",
    "Prediction_Over=model_R_Over.predict(X_test)\n",
    "\n",
    "train_Prediction_Over =model_R_Over.predict(X_train_oversampled)\n",
    "\n",
    "\n",
    "train_accuracy_Over=accuracy_score(y_train_oversampled,train_Prediction_Over)\n",
    "\n",
    "accuracy_Over = accuracy_score(y_test, Prediction_Over)\n",
    "\n",
    "\n",
    "print('Train_Accuracy:', train_accuracy_Over)\n",
    "print('Accuracy:', accuracy_Over)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RMSE: 6192363805.63255\n",
      "Training RMSE: 0.3320804732743398\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing and training data, and calculate RMSE\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_train_pred = model.predict(X_train)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "\n",
    "print('Testing RMSE:', test_rmse)\n",
    "print('Training RMSE:', train_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8652763985435287\n",
      "Train Accuract: 0.8661534641172088\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter=1000,random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "log_predict=logreg.predict(X_test)\n",
    "log_score=accuracy_score(y_test,log_predict)\n",
    "\n",
    "train_log_predict=logreg.predict(X_train)\n",
    "train_log_score=accuracy_score(y_train,train_log_predict)\n",
    "\n",
    "print(\"Accuracy:\",log_score)\n",
    "print(\"Train Accuract:\",train_log_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8507116848725588\n",
      "Train Accuracy: 0.8455344977084107\n"
     ]
    }
   ],
   "source": [
    "logreg_Over = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg_Over.fit(X_train_oversampled, y_train_oversampled)\n",
    "log_predict_Over = logreg_Over.predict(X_test)\n",
    "log_score_Over = accuracy_score(y_test, log_predict_Over)\n",
    "\n",
    "train_log_predict_Over = logreg_Over.predict(X_train_oversampled)\n",
    "train_log_score_Over = accuracy_score(y_train_oversampled, train_log_predict_Over)\n",
    "\n",
    "print(\"Accuracy:\", log_score_Over)\n",
    "print(\"Train Accuracy:\", train_log_score_Over)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree score:  0.8480635551142006\n",
      "Train_Decision Tree score:  0.9125072427779157\n"
     ]
    }
   ],
   "source": [
    "Dec_model=DecisionTreeClassifier(random_state=42)\n",
    "Dec_model.fit(X_train,y_train)\n",
    "Dec_predict=Dec_model.predict(X_test)\n",
    "Train_Dec_predict=Dec_model.predict(X_train)\n",
    "Dec_accuracy=accuracy_score(y_test,Dec_predict)\n",
    "Train_Dec_accuracy=accuracy_score(y_train,Train_Dec_predict)\n",
    "print(\"Decision Tree score: \",Dec_accuracy)\n",
    "print(\"Train_Decision Tree score: \",Train_Dec_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree score on testing data:  0.8278715657067196\n",
      "Decision Tree score on training data (oversampled):  0.892914653784219\n"
     ]
    }
   ],
   "source": [
    "Dec_model_Over = DecisionTreeClassifier(random_state=42)\n",
    "Dec_model_Over.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "Dec_predict_Over = Dec_model_Over.predict(X_test)\n",
    "\n",
    "# Make predictions on the training data (oversampled)\n",
    "Train_Dec_predict_Over = Dec_model_Over.predict(X_train_oversampled)\n",
    "\n",
    "# Calculate accuracy scores\n",
    "Dec_accuracy_Over = accuracy_score(y_test, Dec_predict_Over)\n",
    "Train_Dec_accuracy_Over = accuracy_score(y_train_oversampled, Train_Dec_predict_Over)\n",
    "\n",
    "print(\"Decision Tree score on testing data: \", Dec_accuracy_Over)\n",
    "print(\"Decision Tree score on training data (oversampled): \", Train_Dec_accuracy_Over)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error 0.6765971532605097\n",
      "TestError: 0.6682393841569406\n"
     ]
    }
   ],
   "source": [
    "dummy_class = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_class.fit(X_train,y_train)\n",
    "y_predict=dummy_class.predict(X_test)\n",
    "y_train_predict=dummy_class.predict(X_train)\n",
    "Dummy_accuracy = accuracy_score(y_test, y_predict)\n",
    "Dummy_train_accuracy = accuracy_score(y_train, y_train_predict)\n",
    "print(\"Training error\",Dummy_accuracy)\n",
    "print(\"TestError:\", Dummy_train_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7871565706719629\n"
     ]
    }
   ],
   "source": [
    "Dec_model_OneR=DecisionTreeClassifier(random_state=42,max_depth=1)\n",
    "Dec_model_OneR.fit(train_data[\"SEARCHED_FLAG\"].values.reshape(-1, 1), train_data[\"SUSPECT_ARRESTED_FLAG\"])\n",
    "predictions = Dec_model_OneR.predict(test_data[\"SEARCHED_FLAG\"].values.reshape(-1, 1))\n",
    "accuracy_OneR = accuracy_score(test_data[\"SUSPECT_ARRESTED_FLAG\"], predictions)\n",
    "print(\"Accuracy:\", accuracy_OneR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier 0.86329030122476\n"
     ]
    }
   ],
   "source": [
    "estimators=[(\"logreg\", logreg), (\"rf\", model_R), (\"Dc\", Dec_model),(\"Naive\",Model_N)]\n",
    "#create our voting classifier, inputting our models\n",
    "ensemble = VotingClassifier(estimators)\n",
    "ensemble.fit(X_train, y_train)\n",
    "#test our model on the test data\n",
    "print(\"Voting Classifier\",ensemble.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient  0.8702416418404502\n",
      "Train Gradient  0.8659879149077063\n"
     ]
    }
   ],
   "source": [
    "G_Model=GradientBoostingClassifier(random_state=42) #look for the base alg !! Gradient Boosted Decision Trees?\n",
    "G_Model.fit(X_train,y_train)\n",
    "G_predict=G_Model.predict(X_test)\n",
    "G_predict_train=G_Model.predict(X_train)\n",
    "G_accracy=accuracy_score(y_test,G_predict)\n",
    "G_accracy_train=accuracy_score(y_train,G_predict_train)\n",
    "print(\"Gradient \", G_accracy)\n",
    "print(\"Train Gradient \", G_accracy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient  0.8649453823237339\n",
      "Train Gradient  0.8400842313885792\n"
     ]
    }
   ],
   "source": [
    "G_Model=GradientBoostingClassifier(random_state=42) #look for the base alg !! Gradient Boosted Decision Trees?\n",
    "G_Model.fit(X_train_oversampled,y_train_oversampled)\n",
    "G_predict=G_Model.predict(X_test)\n",
    "G_predict_train=G_Model.predict(X_train_oversampled)\n",
    "G_accracy=accuracy_score(y_test,G_predict)\n",
    "G_accracy_train=accuracy_score(y_train_oversampled,G_predict_train)\n",
    "print(\"Gradient \", G_accracy)\n",
    "print(\"Train Gradient \", G_accracy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(random_state=42) \n",
      "\n",
      "Cross-Validation Accuracy: 0.8486057301071057 \n",
      " \n",
      "\n",
      "RandomForestClassifier(random_state=42) \n",
      "\n",
      "Cross-Validation Accuracy: 0.8486057301071057 \n",
      " \n",
      "\n",
      "LogisticRegression(max_iter=1000, random_state=0) \n",
      "\n",
      "Cross-Validation Accuracy: 0.861104051873183 \n",
      " \n",
      "\n",
      "DecisionTreeClassifier(random_state=42) \n",
      "\n",
      "Cross-Validation Accuracy: 0.8401626722415999 \n",
      " \n",
      "\n",
      "MultinomialNB() \n",
      "\n",
      "Cross-Validation Accuracy: 0.8218689634953297 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [RandomForestClassifier(random_state=42),RandomForestClassifier(random_state=42),\n",
    "            LogisticRegression(random_state=0,max_iter=1000),DecisionTreeClassifier(random_state=42)#,MultinomialNB()\n",
    "         ]\n",
    "for model in models:\n",
    "    print(model, '\\n')\n",
    "    score = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=5).mean()\n",
    "    print('Cross-Validation Accuracy:', score, '\\n', '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
