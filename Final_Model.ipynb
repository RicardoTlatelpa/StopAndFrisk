{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix,precision_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "# for ensemble method use\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectKBest,f_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "#TO DO-\n",
    "#One-R\n",
    "# sampling SOMTE\n",
    "\n",
    "\n",
    "# height removed\n",
    "Test_Target = [\"SUSPECT_ARRESTED_FLAG\", \"STOP_WAS_INITIATED\",\n",
    "                 \"OBSERVED_DURATION_MINUTES\", \"SUSPECTED_CRIME_DESCRIPTION\",\n",
    "                 \"STOP_DURATION_MINUTES\", \"FIREARM_FLAG\",\n",
    "                 \"SUSPECT_RACE_DESCRIPTION\",\n",
    "                 \"DEMEANOR_OF_PERSON_STOPPED\", \"SUSPECT_BODY_BUILD_TYPE\",\"FRISKED_FLAG\",\"SEARCHED_FLAG\",\"STOP_LOCATION_BORO_NAME\"]\n",
    "Test1 = pd.read_excel(\"sqf2022.xlsx\", usecols=Test_Target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_test=[\"FIREARM_FLAG\",\"FRISKED_FLAG\",\"SEARCHED_FLAG\"]\n",
    "\n",
    "Test1['FIREARM_FLAG'] = Test1['FIREARM_FLAG'].replace('(null)', 'No')\n",
    "Test1.replace('(null)', np.nan, inplace=True)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "Test1[\"SUSPECT_ARRESTED_FLAG\"] = lb.fit_transform(Test1[\"SUSPECT_ARRESTED_FLAG\"])\n",
    "Test1[\"FIREARM_FLAG\"] = lb.fit_transform(Test1[\"FIREARM_FLAG\"])\n",
    "Test1[\"FRISKED_FLAG\"] = lb.fit_transform(Test1[\"FRISKED_FLAG\"])\n",
    "Test1[\"SEARCHED_FLAG\"] = lb.fit_transform(Test1[\"SEARCHED_FLAG\"])\n",
    "\n",
    "\n",
    "Test1[\"SUSPECT_ARRESTED_FLAG\"].fillna(Test1[\"SUSPECT_ARRESTED_FLAG\"].median,inplace=True)\n",
    "Test1[\"FIREARM_FLAG\"].fillna(Test1[\"FIREARM_FLAG\"].median,inplace=True)\n",
    "Test1[\"FRISKED_FLAG\"].fillna(Test1[\"FRISKED_FLAG\"].median,inplace=True)\n",
    "Test1[\"SEARCHED_FLAG\"].fillna(Test1[\"SEARCHED_FLAG\"].median,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One Hot-Encoding\n",
    "OneHot_Encode_feature = [\"STOP_WAS_INITIATED\",\"SUSPECTED_CRIME_DESCRIPTION\"]\n",
    "Label_En = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "Feature_Encoded = Label_En.fit_transform(Test1[OneHot_Encode_feature])\n",
    "Label_df = pd.DataFrame(Feature_Encoded.toarray(), columns=Label_En.get_feature_names_out(OneHot_Encode_feature))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding\n",
    "LE=LabelEncoder()\n",
    "Test1['SUSPECT_BODY_BUILD_TYPE']=LE.fit_transform(Test1[\"SUSPECT_BODY_BUILD_TYPE\"])\n",
    "Test1['SUSPECT_BODY_BUILD_TYPE'].fillna(Test1['SUSPECT_BODY_BUILD_TYPE'].mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       agitated  angry  annoyed  calm  compliant  cooperative  nervous  \\\n",
      "0             0      0        0     0          0            0        0   \n",
      "1             0      0        0     0          0            0        0   \n",
      "2             0      0        0     0          0            0        1   \n",
      "3             0      0        0     1          0            0        0   \n",
      "4             0      0        0     1          0            0        0   \n",
      "...         ...    ...      ...   ...        ...          ...      ...   \n",
      "15097         0      0        0     0          0            0        0   \n",
      "15098         0      0        0     1          0            0        0   \n",
      "15099         0      0        0     0          1            0        0   \n",
      "15100         0      0        1     0          0            0        0   \n",
      "15101         0      0        1     0          1            0        0   \n",
      "\n",
      "       neutral  normal  upset  \n",
      "0            0       0      0  \n",
      "1            1       0      0  \n",
      "2            0       0      0  \n",
      "3            0       0      0  \n",
      "4            0       0      0  \n",
      "...        ...     ...    ...  \n",
      "15097        1       0      0  \n",
      "15098        0       0      0  \n",
      "15099        0       0      0  \n",
      "15100        0       0      0  \n",
      "15101        0       0      0  \n",
      "\n",
      "[15102 rows x 10 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shion/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Find a way to limit, max featrue, \n",
    "\n",
    "\n",
    "\n",
    "Test1['DEMEANOR_OF_PERSON_STOPPED'].fillna('neutral', inplace=True)\n",
    "texts=Test1['DEMEANOR_OF_PERSON_STOPPED'].tolist()\n",
    "stop_words = ['the', 'is', 'and', 'to', 'of']\n",
    "\n",
    "# # Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=10,stop_words=stop_words)\n",
    "\n",
    "# # Fit and transform the texts using the CountVectorizer\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# # Get the feature names\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# # Convert the vectorized data to a DataFrame\n",
    "vectorized_data = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "\n",
    "# # Print the vectorized data\n",
    "print(vectorized_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15102\n",
      "15102\n",
      "15102\n",
      "Number of null values in 'DEMEANOR_OF_PERSON_STOPPED': 0\n"
     ]
    }
   ],
   "source": [
    "nlpp = pd.read_excel(\"sqf2022.xlsx\", usecols=[\"DEMEANOR_OF_PERSON_STOPPED\"])\n",
    "print(len(nlpp))\n",
    "nlp = nlpp.fillna(value='neutral')\n",
    "print(len(nlp))\n",
    "\n",
    "# Assuming df is your DataFrame and 'column_name' is the name of the column you want to check\n",
    "null_count = nlp['DEMEANOR_OF_PERSON_STOPPED'].isnull().sum()\n",
    "print(len(nlp))\n",
    "print(f\"Number of null values in 'DEMEANOR_OF_PERSON_STOPPED': {null_count}\")\n",
    "data = nlp['DEMEANOR_OF_PERSON_STOPPED'].tolist()\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "new_data = [analyzer.polarity_scores(element)['compound'] for element in data]\n",
    "new = [(pair, compound) for pair, compound in zip(data, new_data)]\n",
    "# Create DataFrame from list of tuples\n",
    "final_df = pd.DataFrame(new, columns=['DEMEANOR_OF_PERSON_STOPPED', 'COMPOUND_SENTIMENT'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Test1 = pd.concat([Test1, Label_df,vectorized_data], axis=1)\n",
    "Test1 = pd.concat([Test1, Label_df], axis=1)\n",
    "#Test1=pd.concat([Test1,vectorized_data],axis=1)\n",
    "#Test1 = pd.concat([Test1,Label_df, final_df], axis=1)\n",
    "\n",
    "\n",
    "#create instance of minmax scale\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "# fir the data in \n",
    "#Test1[\"COMPOUND_SENTIMENT\"]= scaler.fit_transform(Test1['COMPOUND_SENTIMENT'])\n",
    "#cat_feature=list(Label_df.columns)+Feature_test+list(vectorized_data.columns)\n",
    "\n",
    "#cat_feature=list(Label_df.columns)+Feature_test+[\"COMPOUND_SENTIMENT\"]\n",
    "\n",
    "cat_feature=list(Label_df.columns)+Feature_test\n",
    "\n",
    "# drop the original categorical feature column\n",
    "#Test1 = Test1.drop(columns=OneHot_Encode_feature)\n",
    "#Test1= Test1.drop(columns=\"DEMEANOR_OF_PERSON_STOPPED\")\n",
    "\n",
    "Test1.to_csv('output.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data,test_data=train_test_split(Test1,test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "#classi\n",
    "X_train = train_data[cat_feature] # training features_for Random forest\n",
    "y_train = train_data['SUSPECT_ARRESTED_FLAG'] # training target variable for random forest\n",
    "\n",
    "X_test = test_data[cat_feature] # testing features\n",
    "y_test = test_data['SUSPECT_ARRESTED_FLAG'] # testing target variable\n",
    "\n",
    "\n",
    "smote = SMOTE()\n",
    "X_train_oversampled, y_train_oversampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8391261171797418\n",
      "Train_Accuracy: 0.8370168032447645\n",
      "Confusion Matrix:\n",
      "[[1858  186]\n",
      " [ 300  677]]\n",
      "Precision: 0.8227275802559556\n",
      "Recall: 0.8009697604592516\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.88      2044\n",
      "           1       0.78      0.69      0.74       977\n",
      "\n",
      "    accuracy                           0.84      3021\n",
      "   macro avg       0.82      0.80      0.81      3021\n",
      "weighted avg       0.84      0.84      0.84      3021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Model_N=MultinomialNB()\n",
    "Model_N.fit(X_train,y_train)\n",
    "N_prediction=Model_N.predict(X_test)\n",
    "train_N_prediction=Model_N.predict(X_train)\n",
    "N_train_accuracy=accuracy_score(y_train,train_N_prediction)\n",
    "N_accuracy=accuracy_score(y_test,N_prediction)\n",
    "\n",
    "print('Accuracy:', N_accuracy)\n",
    "print('Train_Accuracy:', N_train_accuracy)\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, N_prediction))\n",
    "\n",
    "precision = precision_score(y_test, N_prediction, average='macro')\n",
    "recall = recall_score(y_test, N_prediction, average='macro')\n",
    "\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "\n",
    "classification_report = classification_report(y_test, N_prediction)\n",
    "print('Classification Report:')\n",
    "print(classification_report)\n",
    "\n",
    "#add percision and recall, get the marco avg and weighted avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Accuracy: 0.791898922333705\n",
      "Accuracy: 0.7931148626282688\n",
      "[[1555  489]\n",
      " [ 136  841]]\n",
      "Precision: 0.7759525217538228\n",
      "Recall: 0.8107807858635104\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.76      0.83      2044\n",
      "           1       0.63      0.86      0.73       977\n",
      "\n",
      "    accuracy                           0.79      3021\n",
      "   macro avg       0.78      0.81      0.78      3021\n",
      "weighted avg       0.83      0.79      0.80      3021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "Model_N_Over = MultinomialNB()\n",
    "Model_N_Over.fit(X_train_oversampled, y_train_oversampled)\n",
    "N_prediction_Over = Model_N_Over.predict(X_test)\n",
    "train_N_prediction_Over = Model_N_Over.predict(X_train_oversampled)\n",
    "N_train_accuracy_Over = accuracy_score(y_train_oversampled, train_N_prediction_Over)\n",
    "N_accuracy_Over = accuracy_score(y_test, N_prediction_Over)\n",
    "\n",
    "print('Train_Accuracy:', N_train_accuracy_Over)\n",
    "print('Accuracy:', N_accuracy_Over)\n",
    "print(confusion_matrix(y_test, N_prediction_Over))\n",
    "precision_Over = precision_score(y_test, N_prediction_Over, average='macro')\n",
    "recall_Over = recall_score(y_test, N_prediction_Over, average='macro')\n",
    "\n",
    "print('Precision:', precision_Over)\n",
    "print('Recall:', recall_Over)\n",
    "\n",
    "classification_report = classification_report(y_test, N_prediction_Over)\n",
    "print('Classification Report:')\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Accuracy: 0.8673950831884778\n",
      "Accuracy: 0.8679245283018868\n",
      "[[1877  167]\n",
      " [ 232  745]]\n",
      "Precision: 0.8534406116642959\n",
      "Recall: 0.8404179193865962\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90      2044\n",
      "           1       0.82      0.76      0.79       977\n",
      "\n",
      "    accuracy                           0.87      3021\n",
      "   macro avg       0.85      0.84      0.85      3021\n",
      "weighted avg       0.87      0.87      0.87      3021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model_R = RandomForestClassifier(random_state=42)\n",
    "\n",
    "\n",
    "model_R.fit(X_train,y_train)\n",
    "Prediction=model_R.predict(X_test)\n",
    "\n",
    "train_Prediction =model_R.predict(X_train)\n",
    "\n",
    "\n",
    "train_accuracy=accuracy_score(y_train,train_Prediction)\n",
    "\n",
    "accuracy = accuracy_score(y_test, Prediction)\n",
    "\n",
    "\n",
    "print('Train_Accuracy:', train_accuracy)\n",
    "print('Accuracy:', accuracy)\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test, Prediction))\n",
    "precision = precision_score(y_test, Prediction, average='macro')\n",
    "recall = recall_score(y_test, Prediction, average='macro')\n",
    "\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "\n",
    "classification_report = classification_report(y_test, Prediction)\n",
    "print('Classification Report:')\n",
    "print(classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Accuracy: 0.8409513192121888\n",
      "Accuracy: 0.8579940417080437\n",
      "[[1812  232]\n",
      " [ 197  780]]\n",
      "Precision: 0.8363461262264473\n",
      "Recall: 0.8424296991268851\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89      2044\n",
      "           1       0.77      0.80      0.78       977\n",
      "\n",
      "    accuracy                           0.86      3021\n",
      "   macro avg       0.84      0.84      0.84      3021\n",
      "weighted avg       0.86      0.86      0.86      3021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "model_R_Over = RandomForestClassifier(random_state=42)\n",
    "model_R_Over.fit(X_train_oversampled,y_train_oversampled)\n",
    "Prediction_Over=model_R_Over.predict(X_test)\n",
    "\n",
    "train_Prediction_Over =model_R_Over.predict(X_train_oversampled)\n",
    "\n",
    "\n",
    "train_accuracy_Over=accuracy_score(y_train_oversampled,train_Prediction_Over)\n",
    "\n",
    "accuracy_Over = accuracy_score(y_test, Prediction_Over)\n",
    "\n",
    "\n",
    "print('Train_Accuracy:', train_accuracy_Over)\n",
    "print('Accuracy:', accuracy_Over)\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test, Prediction_Over))\n",
    "precision_Over = precision_score(y_test, Prediction_Over, average='macro')\n",
    "recall_Over = recall_score(y_test, Prediction_Over, average='macro')\n",
    "\n",
    "print('Precision:', precision_Over)\n",
    "print('Recall:', recall_Over)\n",
    "\n",
    "classification_report = classification_report(y_test, Prediction_Over)\n",
    "print('Classification Report:')\n",
    "print(classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8659384309831182\n",
      "Train Accuract: 0.8611042132273818\n",
      "[[1877  167]\n",
      " [ 238  739]]\n",
      "Precision: 0.8515718691778998\n",
      "Recall: 0.8373472950263097\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90      2044\n",
      "           1       0.82      0.76      0.78       977\n",
      "\n",
      "    accuracy                           0.87      3021\n",
      "   macro avg       0.85      0.84      0.84      3021\n",
      "weighted avg       0.86      0.87      0.86      3021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "logreg = LogisticRegression(max_iter=1000,random_state=42)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "log_predict=logreg.predict(X_test)\n",
    "log_score=accuracy_score(y_test,log_predict)\n",
    "\n",
    "train_log_predict=logreg.predict(X_train)\n",
    "train_log_score=accuracy_score(y_train,train_log_predict)\n",
    "\n",
    "print(\"Accuracy:\",log_score)\n",
    "print(\"Train Accuract:\",train_log_score)\n",
    "\n",
    "print(confusion_matrix(y_test, log_predict))\n",
    "precision = precision_score(y_test, log_predict, average='macro')\n",
    "recall = recall_score(y_test, log_predict, average='macro')\n",
    "\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "\n",
    "classification_report = classification_report(y_test, log_predict)\n",
    "print('Classification Report:')\n",
    "print(classification_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.864614366103939\n",
      "Train Accuracy: 0.8371113588504893\n",
      "[[1847  197]\n",
      " [ 212  765]]\n",
      "Precision: 0.8461278460064279\n",
      "Recall: 0.843314782061785\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90      2044\n",
      "           1       0.80      0.78      0.79       977\n",
      "\n",
      "    accuracy                           0.86      3021\n",
      "   macro avg       0.85      0.84      0.84      3021\n",
      "weighted avg       0.86      0.86      0.86      3021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "logreg_Over = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logreg_Over.fit(X_train_oversampled, y_train_oversampled)\n",
    "log_predict_Over = logreg_Over.predict(X_test)\n",
    "log_score_Over = accuracy_score(y_test, log_predict_Over)\n",
    "\n",
    "train_log_predict_Over = logreg_Over.predict(X_train_oversampled)\n",
    "train_log_score_Over = accuracy_score(y_train_oversampled, train_log_predict_Over)\n",
    "\n",
    "print(\"Accuracy:\", log_score_Over)\n",
    "print(\"Train Accuracy:\", train_log_score_Over)\n",
    "\n",
    "print(confusion_matrix(y_test, log_predict_Over))\n",
    "precision_Over = precision_score(y_test, log_predict_Over, average='macro')\n",
    "recall_Over = recall_score(y_test, log_predict_Over, average='macro')\n",
    "\n",
    "print('Precision:', precision_Over)\n",
    "print('Recall:', recall_Over)\n",
    "\n",
    "classification_report = classification_report(y_test, log_predict_Over)\n",
    "print('Classification Report:')\n",
    "print(classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree score:  0.8652763985435287\n",
      "Train_Decision Tree score:  0.8673950831884778\n",
      "[[1880  164]\n",
      " [ 243  734]]\n",
      "Precision: 0.8514556343871922\n",
      "Recall: 0.8355222965786474\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90      2044\n",
      "           1       0.82      0.75      0.78       977\n",
      "\n",
      "    accuracy                           0.87      3021\n",
      "   macro avg       0.85      0.84      0.84      3021\n",
      "weighted avg       0.86      0.87      0.86      3021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Dec_model=DecisionTreeClassifier(random_state=42)\n",
    "Dec_model.fit(X_train,y_train)\n",
    "Dec_predict=Dec_model.predict(X_test)\n",
    "Train_Dec_predict=Dec_model.predict(X_train)\n",
    "Dec_accuracy=accuracy_score(y_test,Dec_predict)\n",
    "Train_Dec_accuracy=accuracy_score(y_train,Train_Dec_predict)\n",
    "print(\"Decision Tree score: \",Dec_accuracy)\n",
    "print(\"Train_Decision Tree score: \",Train_Dec_accuracy)\n",
    "\n",
    "print(confusion_matrix(y_test, Dec_predict))\n",
    "precision = precision_score(y_test, Dec_predict, average='macro')\n",
    "recall = recall_score(y_test, Dec_predict, average='macro')\n",
    "\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "\n",
    "classification_report = classification_report(y_test, Dec_predict)\n",
    "print('Classification Report:')\n",
    "print(classification_report)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree score on testing data:  0.8560079443892751\n",
      "Decision Tree score on training data (oversampled):  0.8409513192121888\n",
      "[[1812  232]\n",
      " [ 203  774]]\n",
      "Precision: 0.8343196404698361\n",
      "Recall: 0.8393590747665984\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89      2044\n",
      "           1       0.77      0.79      0.78       977\n",
      "\n",
      "    accuracy                           0.86      3021\n",
      "   macro avg       0.83      0.84      0.84      3021\n",
      "weighted avg       0.86      0.86      0.86      3021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Dec_model_Over = DecisionTreeClassifier(random_state=42)\n",
    "Dec_model_Over.fit(X_train_oversampled, y_train_oversampled)\n",
    "\n",
    "# Make predictions on the testing data\n",
    "Dec_predict_Over = Dec_model_Over.predict(X_test)\n",
    "\n",
    "# Make predictions on the training data (oversampled)\n",
    "Train_Dec_predict_Over = Dec_model_Over.predict(X_train_oversampled)\n",
    "\n",
    "# Calculate accuracy scores\n",
    "Dec_accuracy_Over = accuracy_score(y_test, Dec_predict_Over)\n",
    "Train_Dec_accuracy_Over = accuracy_score(y_train_oversampled, Train_Dec_predict_Over)\n",
    "\n",
    "print(\"Decision Tree score on testing data: \", Dec_accuracy_Over)\n",
    "print(\"Decision Tree score on training data (oversampled): \", Train_Dec_accuracy_Over)\n",
    "\n",
    "print(confusion_matrix(y_test, Dec_predict_Over))\n",
    "precision_Over = precision_score(y_test, Dec_predict_Over, average='macro')\n",
    "recall_Over = recall_score(y_test, Dec_predict_Over, average='macro')\n",
    "\n",
    "print('Precision:', precision_Over)\n",
    "print('Recall:', recall_Over)\n",
    "\n",
    "classification_report = classification_report(y_test, Dec_predict_Over)\n",
    "print('Classification Report:')\n",
    "print(classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.6765971532605097\n",
      "training_score : 0.6682393841569406\n",
      "[[2044    0]\n",
      " [ 977    0]]\n",
      "Precision: 0.33829857663025487\n",
      "Recall: 0.5\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      1.00      0.81      2044\n",
      "           1       0.00      0.00      0.00       977\n",
      "\n",
      "    accuracy                           0.68      3021\n",
      "   macro avg       0.34      0.50      0.40      3021\n",
      "weighted avg       0.46      0.68      0.55      3021\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shion/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/shion/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/shion/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/shion/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dummy_class = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_class.fit(X_train,y_train)\n",
    "y_predict=dummy_class.predict(X_test)\n",
    "y_train_predict=dummy_class.predict(X_train)\n",
    "Dummy_accuracy = accuracy_score(y_test, y_predict)\n",
    "Dummy_train_accuracy = accuracy_score(y_train, y_train_predict)\n",
    "print(\"Score: \",Dummy_accuracy)\n",
    "print(\"training_score :\", Dummy_train_accuracy)\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test, y_predict))\n",
    "precision = precision_score(y_test, y_predict, average='macro')\n",
    "recall = recall_score(y_test, y_predict, average='macro')\n",
    "\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "\n",
    "classification_report = classification_report(y_test, y_predict)\n",
    "print('Classification Report:')\n",
    "print(classification_report)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.6765971532605097\n",
      "training_score: 0.5\n",
      "[[2044    0]\n",
      " [ 977    0]]\n",
      "Precision: 0.33829857663025487\n",
      "Recall: 0.5\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      1.00      0.81      2044\n",
      "           1       0.00      0.00      0.00       977\n",
      "\n",
      "    accuracy                           0.68      3021\n",
      "   macro avg       0.34      0.50      0.40      3021\n",
      "weighted avg       0.46      0.68      0.55      3021\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shion/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/shion/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/shion/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/shion/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dummy_class_over = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_class_over.fit(X_train_oversampled,y_train_oversampled)\n",
    "y_predict_over=dummy_class_over.predict(X_test)\n",
    "y_train_predict_over=dummy_class_over.predict(X_train_oversampled)\n",
    "Dummy_accuracy_over = accuracy_score(y_test, y_predict_over)\n",
    "Dummy_train_accuracy_over = accuracy_score(y_train_oversampled, y_train_predict_over)\n",
    "print(\"Score: \",Dummy_accuracy_over)\n",
    "print(\"training_score:\", Dummy_train_accuracy_over)\n",
    "\n",
    "print(confusion_matrix(y_test, y_predict_over))\n",
    "precision_Over = precision_score(y_test, y_predict_over, average='macro')\n",
    "recall_Over= recall_score(y_test, y_predict_over, average='macro')\n",
    "\n",
    "print('Precision:', precision_Over)\n",
    "print('Recall:', recall_Over)\n",
    "\n",
    "classification_report = classification_report(y_test, y_predict_over)\n",
    "print('Classification Report:')\n",
    "print(classification_report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7871565706719629\n",
      "Training 0.7859448721132356\n",
      "[[1571  473]\n",
      " [ 170  807]]\n",
      "Precision: 0.7664118592044802\n",
      "Recall: 0.797294475480073\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.77      0.83      2044\n",
      "           1       0.63      0.83      0.72       977\n",
      "\n",
      "    accuracy                           0.79      3021\n",
      "   macro avg       0.77      0.80      0.77      3021\n",
      "weighted avg       0.81      0.79      0.79      3021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Dec_model_OneR=DecisionTreeClassifier(random_state=42,max_depth=1)\n",
    "Dec_model_OneR.fit(train_data[\"SEARCHED_FLAG\"].values.reshape(-1, 1), train_data[\"SUSPECT_ARRESTED_FLAG\"])\n",
    "predictions = Dec_model_OneR.predict(test_data[\"SEARCHED_FLAG\"].values.reshape(-1, 1))\n",
    "accuracy_OneR = accuracy_score(test_data[\"SUSPECT_ARRESTED_FLAG\"], predictions)\n",
    "\n",
    "predictions_training=Dec_model_OneR.predict(train_data[\"SEARCHED_FLAG\"].values.reshape(-1, 1))\n",
    "accuracy_OneR_training=accuracy_score(train_data[\"SUSPECT_ARRESTED_FLAG\"], predictions_training)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_OneR)\n",
    "print(\"Training\", accuracy_OneR_training)\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "recall= recall_score(y_test, predictions, average='macro')\n",
    "\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "\n",
    "classification_report = classification_report(y_test, predictions)\n",
    "print('Classification Report:')\n",
    "print(classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7871565706719629\n",
      "[[1571  473]\n",
      " [ 170  807]]\n",
      "Precision: 0.7664118592044802\n",
      "Recall: 0.797294475480073\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.77      0.83      2044\n",
      "           1       0.63      0.83      0.72       977\n",
      "\n",
      "    accuracy                           0.79      3021\n",
      "   macro avg       0.77      0.80      0.77      3021\n",
      "weighted avg       0.81      0.79      0.79      3021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "Dec_model_OneR=DecisionTreeClassifier(random_state=42,max_depth=1)\n",
    "Dec_model_OneR.fit(X_train_oversampled[\"SEARCHED_FLAG\"].values.reshape(-1, 1), y_train_oversampled)\n",
    "predictions_Over = Dec_model_OneR.predict(test_data[\"SEARCHED_FLAG\"].values.reshape(-1, 1))\n",
    "accuracy_OneR = accuracy_score(test_data[\"SUSPECT_ARRESTED_FLAG\"], predictions_Over)\n",
    "print(\"Accuracy:\", accuracy_OneR)\n",
    "\n",
    "print(confusion_matrix(y_test, predictions_Over))\n",
    "precision_Over = precision_score(y_test, predictions_Over, average='macro')\n",
    "recall_Over= recall_score(y_test, predictions_Over, average='macro')\n",
    "\n",
    "print('Precision:', precision_Over)\n",
    "print('Recall:', recall_Over)\n",
    "\n",
    "classification_report = classification_report(y_test, predictions_Over)\n",
    "print('Classification Report:')\n",
    "print(classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier 0.8682555445216815\n",
      "[[1887  157]\n",
      " [ 241  736]]\n",
      "Precision: 0.855468125099984\n",
      "Recall: 0.8382581667991995\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90      2044\n",
      "           1       0.82      0.75      0.79       977\n",
      "\n",
      "    accuracy                           0.87      3021\n",
      "   macro avg       0.86      0.84      0.85      3021\n",
      "weighted avg       0.87      0.87      0.87      3021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "estimators=[(\"logreg\", logreg), (\"rf\", model_R), (\"Dc\", Dec_model),(\"Naive\",Model_N)]\n",
    "#create our voting classifier, inputting our models\n",
    "ensemble = VotingClassifier(estimators)\n",
    "ensemble.fit(X_train, y_train)\n",
    "#test our model on the test data\n",
    "\n",
    "y_pred = ensemble.predict(X_test)\n",
    "\n",
    "print(\"Voting Classifier\",ensemble.score(X_test, y_test))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall= recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "\n",
    "classification_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier 0.8639523336643495\n",
      "[[1841  203]\n",
      " [ 208  769]]\n",
      "Precision: 0.8448196651181847\n",
      "Recall: 0.8438941545968228\n",
      "Voting Classifier 0.8639523336643495\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90      2044\n",
      "           1       0.79      0.79      0.79       977\n",
      "\n",
      "    accuracy                           0.86      3021\n",
      "   macro avg       0.84      0.84      0.84      3021\n",
      "weighted avg       0.86      0.86      0.86      3021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "estimators=[(\"logreg\", logreg), (\"rf\", model_R), (\"Dc\", Dec_model),(\"Naive\",Model_N)]\n",
    "#create our voting classifier, inputting our models\n",
    "ensemble = VotingClassifier(estimators)\n",
    "ensemble.fit(X_train_oversampled, y_train_oversampled)\n",
    "#test our model on the test data\n",
    "print(\"Voting Classifier\",ensemble.score(X_test, y_test))\n",
    "\n",
    "y_pred_Over = ensemble.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_Over))\n",
    "precision_Over = precision_score(y_test, y_pred_Over, average='macro')\n",
    "recall_Over= recall_score(y_test, y_pred_Over, average='macro')\n",
    "\n",
    "print('Precision:', precision_Over)\n",
    "print('Recall:', recall_Over)\n",
    "\n",
    "\n",
    "print(\"Voting Classifier\",ensemble.score(X_test, y_test))\n",
    "\n",
    "classification_report = classification_report(y_test, y_pred_Over)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient  0.8702416418404502\n",
      "Train Gradient  0.8659879149077063\n",
      "[[1882  162]\n",
      " [ 230  747]]\n",
      "Precision: 0.8564403315331532\n",
      "Recall: 0.8426645528165417\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.91      2044\n",
      "           1       0.82      0.76      0.79       977\n",
      "\n",
      "    accuracy                           0.87      3021\n",
      "   macro avg       0.86      0.84      0.85      3021\n",
      "weighted avg       0.87      0.87      0.87      3021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "G_Model=GradientBoostingClassifier(random_state=42) #Gradient Boosted Decision Trees\n",
    "G_Model.fit(X_train,y_train)\n",
    "G_predict=G_Model.predict(X_test)\n",
    "G_predict_train=G_Model.predict(X_train)\n",
    "G_accracy=accuracy_score(y_test,G_predict)\n",
    "G_accracy_train=accuracy_score(y_train,G_predict_train)\n",
    "print(\"Gradient \", G_accracy)\n",
    "print(\"Train Gradient \", G_accracy_train)\n",
    "\n",
    "print(confusion_matrix(y_test, G_predict))\n",
    "precision = precision_score(y_test, G_predict, average='macro')\n",
    "recall= recall_score(y_test, G_predict, average='macro')\n",
    "\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "\n",
    "\n",
    "classification_report = classification_report(y_test, G_predict)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient  0.8642833498841443\n",
      "Train Gradient  0.8376687724513812\n",
      "[[1846  198]\n",
      " [ 212  765]]\n",
      "Precision: 0.8456899448698036\n",
      "Recall: 0.8430701636664817\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90      2044\n",
      "           1       0.79      0.78      0.79       977\n",
      "\n",
      "    accuracy                           0.86      3021\n",
      "   macro avg       0.85      0.84      0.84      3021\n",
      "weighted avg       0.86      0.86      0.86      3021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "G_Model=GradientBoostingClassifier(random_state=42) #Gradient Boosted Decision Trees\n",
    "G_Model.fit(X_train_oversampled,y_train_oversampled)\n",
    "G_predict=G_Model.predict(X_test)\n",
    "G_predict_train=G_Model.predict(X_train_oversampled)\n",
    "G_accracy=accuracy_score(y_test,G_predict)\n",
    "G_accracy_train=accuracy_score(y_train_oversampled,G_predict_train)\n",
    "print(\"Gradient \", G_accracy)\n",
    "print(\"Train Gradient \", G_accracy_train)\n",
    "\n",
    "\n",
    "\n",
    "print(confusion_matrix(y_test, G_predict))\n",
    "precision = precision_score(y_test, G_predict, average='macro')\n",
    "recall= recall_score(y_test, G_predict, average='macro')\n",
    "\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "\n",
    "\n",
    "classification_report = classification_report(y_test, G_predict)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [RandomForestClassifier(random_state=42),RandomForestClassifier(random_state=42),\n",
    "#             LogisticRegression(random_state=0,max_iter=1000),DecisionTreeClassifier(random_state=42),MultinomialNB()\n",
    "#          ]\n",
    "# for model in models:\n",
    "#     print(model, '\\n')\n",
    "#     score = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=5).mean()\n",
    "#     print('Cross-Validation Accuracy:', score, '\\n', '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
