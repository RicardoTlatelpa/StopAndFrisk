{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N    10117\n",
      "Y     4985\n",
      "Name: SUSPECT_ARRESTED_FLAG, dtype: int64\n",
      "STOP_WAS_INITIATED                0\n",
      "OBSERVED_DURATION_MINUTES         0\n",
      "SUSPECTED_CRIME_DESCRIPTION       0\n",
      "STOP_DURATION_MINUTES             0\n",
      "SUSPECT_ARRESTED_FLAG             0\n",
      "FRISKED_FLAG                      0\n",
      "SEARCHED_FLAG                     0\n",
      "FIREARM_FLAG                      0\n",
      "DEMEANOR_OF_PERSON_STOPPED     1907\n",
      "SUSPECT_RACE_DESCRIPTION        192\n",
      "SUSPECT_BODY_BUILD_TYPE         573\n",
      "STOP_LOCATION_BORO_NAME           0\n",
      "dtype: int64\n",
      "new STOP_WAS_INITIATED                0\n",
      "OBSERVED_DURATION_MINUTES         0\n",
      "SUSPECTED_CRIME_DESCRIPTION       0\n",
      "STOP_DURATION_MINUTES             0\n",
      "SUSPECT_ARRESTED_FLAG             0\n",
      "FRISKED_FLAG                      0\n",
      "SEARCHED_FLAG                     0\n",
      "FIREARM_FLAG                      0\n",
      "DEMEANOR_OF_PERSON_STOPPED     1907\n",
      "SUSPECT_RACE_DESCRIPTION          0\n",
      "SUSPECT_BODY_BUILD_TYPE           0\n",
      "STOP_LOCATION_BORO_NAME           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# for ensemble method use\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "dummy_reg = DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "# height removed\n",
    "Test_Target = [\"SUSPECT_ARRESTED_FLAG\", \"STOP_WAS_INITIATED\",\n",
    "                 \"OBSERVED_DURATION_MINUTES\", \"SUSPECTED_CRIME_DESCRIPTION\",\n",
    "                 \"STOP_DURATION_MINUTES\", \"FIREARM_FLAG\",\n",
    "                 \"SUSPECT_RACE_DESCRIPTION\",\n",
    "                 \"DEMEANOR_OF_PERSON_STOPPED\", \"SUSPECT_BODY_BUILD_TYPE\",\"FRISKED_FLAG\",\"SEARCHED_FLAG\",\"STOP_LOCATION_BORO_NAME\"]\n",
    "Test1 = pd.read_excel(\"sqf2022.xlsx\", usecols=Test_Target)\n",
    "\n",
    "#print('MAX1:', Test1[\"OBSERVED_DURATION_MINUTES\"].max())\n",
    "\n",
    "#MS = Test1[\"STOP_DURATION_MINUTES\"].values.reshape(-1, 1)\n",
    "#scaler2 = StandardScaler()\n",
    "#z_scores2 = scaler2.fit_transform(MS)\n",
    "\n",
    "#outliers2 = np.where(z_scores2 > 1.5)\n",
    "#Test1 = Test1.drop(outliers2[0], axis=0)\n",
    "#print('MAX2:', Test1[\"STOP_DURATION_MINUTES\"].max())\n",
    "\n",
    "\n",
    "\n",
    "Feature_test=[\"FIREARM_FLAG\",\"FRISKED_FLAG\",\"SEARCHED_FLAG\"]\n",
    "#C_Feature_test=[\"STOP_WAS_INITIATED\",\"SUSPECTED_CRIME_DESCRIPTION\",\"SUSPECT_RACE_DESCRIPTION\",\"DEMEANOR_OF_PERSON_STOPPED\"]\n",
    "value_counts = Test1['SUSPECT_ARRESTED_FLAG'].value_counts()\n",
    "print(value_counts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Test1['FIREARM_FLAG'] = Test1['FIREARM_FLAG'].replace('(null)', 'No')\n",
    "Test1.replace('(null)', np.nan, inplace=True)\n",
    "print(Test1.isna().sum())\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "Test1[\"SUSPECT_ARRESTED_FLAG\"] = lb.fit_transform(Test1[\"SUSPECT_ARRESTED_FLAG\"])\n",
    "Test1[\"FIREARM_FLAG\"] = lb.fit_transform(Test1[\"FIREARM_FLAG\"])\n",
    "Test1[\"FRISKED_FLAG\"] = lb.fit_transform(Test1[\"FRISKED_FLAG\"])\n",
    "Test1[\"SEARCHED_FLAG\"] = lb.fit_transform(Test1[\"SEARCHED_FLAG\"])\n",
    "\n",
    "\n",
    "Test1[\"SUSPECT_ARRESTED_FLAG\"].fillna(Test1[\"SUSPECT_ARRESTED_FLAG\"].median,inplace=True)\n",
    "Test1[\"FIREARM_FLAG\"].fillna(Test1[\"FIREARM_FLAG\"].median,inplace=True)\n",
    "Test1[\"FRISKED_FLAG\"].fillna(Test1[\"FRISKED_FLAG\"].median,inplace=True)\n",
    "Test1[\"SEARCHED_FLAG\"].fillna(Test1[\"SEARCHED_FLAG\"].median,inplace=True)\n",
    "\n",
    "mode_val = Test1[\"SUSPECT_RACE_DESCRIPTION\"].mode()[0]\n",
    "Test1[\"SUSPECT_RACE_DESCRIPTION\"].fillna(mode_val, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "Test1.to_csv('output.csv', index=False)\n",
    "#one Hot encoding\n",
    "OneHot_Encode_feature = [\"STOP_WAS_INITIATED\",\"SUSPECTED_CRIME_DESCRIPTION\"]\n",
    "Label_En = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "Feature_Encoded = Label_En.fit_transform(Test1[OneHot_Encode_feature])\n",
    "Label_df = pd.DataFrame(Feature_Encoded.toarray(), columns=Label_En.get_feature_names_out(OneHot_Encode_feature))\n",
    "#Label_df.fillna(value=0, inplace=True)\n",
    "#Label_df.fillna(median)\n",
    "\n",
    "\n",
    "#label encoding\n",
    "LE=LabelEncoder()\n",
    "Test1['SUSPECT_BODY_BUILD_TYPE']=LE.fit_transform(Test1[\"SUSPECT_BODY_BUILD_TYPE\"])\n",
    "Test1['SUSPECT_BODY_BUILD_TYPE'].fillna(Test1['SUSPECT_BODY_BUILD_TYPE'].mean, inplace=True)\n",
    "#test is there still nan\n",
    "print(\"new\",Test1.isna().sum())\n",
    "\n",
    "#cat_feature=list(Label_df.columns)+['SUSPECT_BODY_BUILD_TYPE']\n",
    "#with everything, un comment the below code to include both Categorical data and numeric in RandomForest\n",
    "#cat_feature=list(Label_df.columns)+['SUSPECT_BODY_BUILD_TYPE']+Feature_test\n",
    "cat_feature=list(Label_df.columns)+Feature_test\n",
    "\n",
    "\n",
    "Test1 = pd.concat([Test1, Label_df], axis=1)\n",
    "\n",
    "# drop the original categorical feature column\n",
    "Test1 = Test1.drop(columns=OneHot_Encode_feature)\n",
    "\n",
    "#cols = ['OBSERVED_DURATION_MINUTES', 'STOP_DURATION_MINUTES']\n",
    "\n",
    "# Create a StandardScaler object\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# Standardize the features\n",
    "#Test1[cols] = scaler.fit_transform(Test1[cols])\n",
    "\n",
    "# Define the threshold for outlier detection\n",
    "#threshold = 3\n",
    "\n",
    "# Calculate the absolute z-score for each data point\n",
    "#z = np.abs(Test1[cols].values)\n",
    "\n",
    "# Identify the rows that contain outliers\n",
    "#outliers = np.where(z > threshold)[0]\n",
    "\n",
    "# Remove the rows that contain outliers\n",
    "#Test1 = Test1.drop(outliers, axis=0)\n",
    "\n",
    "# Inverse transform the standardized data to obtain the original values\n",
    "#Test1[cols] = scaler.inverse_transform(Test1[cols])\n",
    "#print(Test1['OBSERVED_DURATION_MINUTES'].max())\n",
    "#print(Test1['STOP_DURATION_MINUTES'].max())\n",
    "\n",
    "train_data,test_data=train_test_split(Test1,test_size=0.2,random_state=42)\n",
    "\n",
    "# TODO: add linear regression to models: LinearRegression()\n",
    "models = [RandomForestClassifier(random_state=42),RandomForestClassifier(random_state=42),\n",
    "            LogisticRegression(random_state=0,max_iter=1000),DecisionTreeClassifier(random_state=42),MultinomialNB()\n",
    "         ]\n",
    "\n",
    "\n",
    "\n",
    "# extract the categorical features and target variable from the training and testing data\n",
    "#Random Forest\n",
    "X_train = train_data[cat_feature] # training features_for Random forest\n",
    "y_train = train_data['SUSPECT_ARRESTED_FLAG'] # training target variable for random forest\n",
    "\n",
    "X_test = test_data[cat_feature] # testing features\n",
    "y_test = test_data['SUSPECT_ARRESTED_FLAG'] # testing target variable\n",
    "\n",
    "#Linear regression\n",
    "X_train_linear=train_data[Feature_test] \n",
    "y_train_linear=train_data[\"SUSPECT_ARRESTED_FLAG\"]\n",
    "\n",
    "X_test_linear=test_data[Feature_test]\n",
    "y_test_linear=test_data['SUSPECT_ARRESTED_FLAG']\n",
    "\n",
    "#count Nan in dataset\n",
    "#print(Test1.isna().sum())\n",
    "# LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "# models \n",
    "Test1.to_csv('output.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "new OBSERVED_DURATION_MINUTES                                                     0\n",
      "STOP_DURATION_MINUTES                                                         0\n",
      "SUSPECT_ARRESTED_FLAG                                                         0\n",
      "FRISKED_FLAG                                                                  0\n",
      "SEARCHED_FLAG                                                                 0\n",
      "FIREARM_FLAG                                                                  0\n",
      "DEMEANOR_OF_PERSON_STOPPED                                                 1907\n",
      "SUSPECT_RACE_DESCRIPTION                                                      0\n",
      "SUSPECT_BODY_BUILD_TYPE                                                       0\n",
      "STOP_LOCATION_BORO_NAME                                                       0\n",
      "STOP_WAS_INITIATED_Based on C/W on Scene                                      0\n",
      "STOP_WAS_INITIATED_Based on Radio Run                                         0\n",
      "STOP_WAS_INITIATED_Based on Self Initiated                                    0\n",
      "SUSPECTED_CRIME_DESCRIPTION_ASSAULT                                           0\n",
      "SUSPECTED_CRIME_DESCRIPTION_AUTO STRIPPIG                                     0\n",
      "SUSPECTED_CRIME_DESCRIPTION_BURGLARY                                          0\n",
      "SUSPECTED_CRIME_DESCRIPTION_CPSP                                              0\n",
      "SUSPECTED_CRIME_DESCRIPTION_CPW                                               0\n",
      "SUSPECTED_CRIME_DESCRIPTION_CRIMINAL MISCHIEF                                 0\n",
      "SUSPECTED_CRIME_DESCRIPTION_CRIMINAL POSSESSION OF CONTROLLED SUBSTANCE       0\n",
      "SUSPECTED_CRIME_DESCRIPTION_CRIMINAL POSSESSION OF FORGED INSTRUMENT          0\n",
      "SUSPECTED_CRIME_DESCRIPTION_CRIMINAL POSSESSION OF MARIHUANA                  0\n",
      "SUSPECTED_CRIME_DESCRIPTION_CRIMINAL SALE OF CONTROLLED SUBSTANCE             0\n",
      "SUSPECTED_CRIME_DESCRIPTION_CRIMINAL SALE OF MARIHUANA                        0\n",
      "SUSPECTED_CRIME_DESCRIPTION_CRIMINAL TRESPASS                                 0\n",
      "SUSPECTED_CRIME_DESCRIPTION_FORCIBLE TOUCHING                                 0\n",
      "SUSPECTED_CRIME_DESCRIPTION_GRAND LARCENY                                     0\n",
      "SUSPECTED_CRIME_DESCRIPTION_GRAND LARCENY AUTO                                0\n",
      "SUSPECTED_CRIME_DESCRIPTION_MAKING GRAFFITI                                   0\n",
      "SUSPECTED_CRIME_DESCRIPTION_MENACING                                          0\n",
      "SUSPECTED_CRIME_DESCRIPTION_MURDER                                            0\n",
      "SUSPECTED_CRIME_DESCRIPTION_OTHER                                             0\n",
      "SUSPECTED_CRIME_DESCRIPTION_PETIT LARCENY                                     0\n",
      "SUSPECTED_CRIME_DESCRIPTION_RAPE                                              0\n",
      "SUSPECTED_CRIME_DESCRIPTION_RECKLESS ENDANGERMENT                             0\n",
      "SUSPECTED_CRIME_DESCRIPTION_ROBBERY                                           0\n",
      "SUSPECTED_CRIME_DESCRIPTION_TERRORISM                                         0\n",
      "SUSPECTED_CRIME_DESCRIPTION_THEFT OF SERVICES                                 0\n",
      "SUSPECTED_CRIME_DESCRIPTION_UNAUTHORIZED USE OF A VEHICLE                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(np.any(np.isnan(X_train)))\n",
    "print(np.any(np.isinf(X_train)))\n",
    "print(np.any(np.isnan(X_train)))\n",
    "print(np.any(np.isinf(X_train)))\n",
    "print(\"new\",Test1.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Accuracy: 0.8391261171797418\n",
      "Accuracy: 0.8370168032447645\n"
     ]
    }
   ],
   "source": [
    "Model_N=MultinomialNB()\n",
    "Model_N.fit(X_train,y_train)\n",
    "N_prediction=Model_N.predict(X_test)\n",
    "train_N_prediction=Model_N.predict(X_train)\n",
    "N_train_accuracy=accuracy_score(y_train,train_N_prediction)\n",
    "N_accuracy=accuracy_score(y_test,N_prediction)\n",
    "\n",
    "print('Train_Accuracy:', N_accuracy)\n",
    "print('Accuracy:', N_train_accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_Accuracy: 0.8673950831884778\n",
      "Accuracy: 0.8679245283018868\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model_R = RandomForestClassifier(random_state=42)\n",
    "#model_R.fit(train_data[Feature_test], train_data[\"SUSPECT_ARRESTED_FLAG\"])\n",
    "\n",
    "#with only caterogical data but transform to numeric/ with both\n",
    "\n",
    "model_R.fit(X_train,y_train)\n",
    "Prediction=model_R.predict(X_test)\n",
    "\n",
    "train_Prediction =model_R.predict(X_train)\n",
    "\n",
    "\n",
    "train_accuracy=accuracy_score(y_train,train_Prediction)\n",
    "\n",
    "accuracy = accuracy_score(y_test, Prediction)\n",
    "\n",
    "\n",
    "print('Train_Accuracy:', train_accuracy)\n",
    "print('Accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing RMSE: 0.34058471641017063\n",
      "Training RMSE: 0.345491521615495\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing and training data, and calculate RMSE\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_train_pred = model.predict(X_train)\n",
    "test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "\n",
    "print('Testing RMSE:', test_rmse)\n",
    "print('Training RMSE:', train_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8659384309831182\n",
      "Train Accuract: 0.8611042132273818\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "log_predict=logreg.predict(X_test)\n",
    "log_score=accuracy_score(y_test,log_predict)\n",
    "\n",
    "train_log_predict=logreg.predict(X_train)\n",
    "train_log_score=accuracy_score(y_train,train_log_predict)\n",
    "\n",
    "print(\"Accuracy:\",log_score)\n",
    "print(\"Train Accuract:\",train_log_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree score:  0.8652763985435287\n",
      "Train_Decision Tree score:  0.8673950831884778\n"
     ]
    }
   ],
   "source": [
    "Dec_model=DecisionTreeClassifier(random_state=42)\n",
    "Dec_model.fit(X_train,y_train)\n",
    "Dec_predict=Dec_model.predict(X_test)\n",
    "Train_Dec_predict=Dec_model.predict(X_train)\n",
    "Dec_accuracy=accuracy_score(y_test,Dec_predict)\n",
    "Train_Dec_accuracy=accuracy_score(y_train,Train_Dec_predict)\n",
    "print(\"Decision Tree score: \",Dec_accuracy)\n",
    "print(\"Train_Decision Tree score: \",Train_Dec_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error 0.47084552628064064\n",
      "TestError: 0.46784965294918474\n"
     ]
    }
   ],
   "source": [
    "dummy_reg=DummyRegressor(strategy=\"mean\")\n",
    "dummy_reg.fit(X_train,y_train)\n",
    "y_train_predict=dummy_reg.predict(X_train)\n",
    "\n",
    "trainerror=mean_squared_error(y_train,y_train_predict,squared=False)\n",
    "\n",
    "\n",
    "y_pred=dummy_reg.predict(X_test)\n",
    "testerror=mean_squared_error(y_test,y_pred,squared=False)\n",
    "\n",
    "print(\"Training error\",trainerror)\n",
    "print(\"TestError:\", testerror)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(random_state=42) \n",
      "\n",
      "Cross-Validation Accuracy: 0.8620978746023613 \n",
      " \n",
      "\n",
      "RandomForestClassifier(random_state=42) \n",
      "\n",
      "Cross-Validation Accuracy: 0.8620978746023613 \n",
      " \n",
      "\n",
      "LogisticRegression(max_iter=1000, random_state=0) \n",
      "\n",
      "Cross-Validation Accuracy: 0.8614354859480446 \n",
      " \n",
      "\n",
      "DecisionTreeClassifier(random_state=42) \n",
      "\n",
      "Cross-Validation Accuracy: 0.860938762956651 \n",
      " \n",
      "\n",
      "MultinomialNB() \n",
      "\n",
      "Cross-Validation Accuracy: 0.8336231426402934 \n",
      " \n",
      "\n",
      "0.8689175769612711\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model, '\\n')\n",
    "    score = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=5).mean()\n",
    "    print('Cross-Validation Accuracy:', score, '\\n', '\\n')\n",
    "\n",
    "estimators=[(\"logreg\", logreg), (\"rf\", model_R), (\"Dc\", Dec_model),(\"Linear\",model),(\"Naive\",Model_N)]\n",
    "#create our voting classifier, inputting our models\n",
    "ensemble = VotingClassifier(estimators)\n",
    "ensemble.fit(X_train, y_train)\n",
    "#test our model on the test data\n",
    "print(ensemble.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpp = pd.read_excel(\"sqf2022.xlsx\", usecols=[\"DEMEANOR_OF_PERSON_STOPPED\"])\n",
    "nlp = nlpp.fillna(value='neutral')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15102\n",
      "Number of null values in 'column_name': 0\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your DataFrame and 'column_name' is the name of the column you want to check\n",
    "null_count = nlp['DEMEANOR_OF_PERSON_STOPPED'].isnull().sum()\n",
    "print(len(nlp))\n",
    "print(f\"Number of null values in 'column_name': {null_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataa = nlp['DEMEANOR_OF_PERSON_STOPPED'].tolist()\n",
    "dataa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "new_data = [ analyzer.polarity_scores(element)['compound'] for element in dataa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = zip(dataa, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "new=[]\n",
    "for pair in data:\n",
    "    new.append(pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from list of tuples\n",
    "final_df = pd.DataFrame(new, columns=['DEMEANOR_OF_PERSON_STOPPED', 'COMPOUND_SENTIMENT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEMEANOR_OF_PERSON_STOPPED</th>\n",
       "      <th>COMPOUND_SENTIMENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FLED ON FOOT</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NERVOUS CHANGING DIRECTION FROM OFFICERS RUNNING</td>\n",
       "      <td>-0.2732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CALM</td>\n",
       "      <td>0.3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CALM</td>\n",
       "      <td>0.3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15097</th>\n",
       "      <td>neutral</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15098</th>\n",
       "      <td>CALM</td>\n",
       "      <td>0.3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15099</th>\n",
       "      <td>COMPLIANT</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15100</th>\n",
       "      <td>ANNOYED</td>\n",
       "      <td>-0.3818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15101</th>\n",
       "      <td>ANNOYED NON COMPLIANT</td>\n",
       "      <td>-0.3818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15102 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             DEMEANOR_OF_PERSON_STOPPED  COMPOUND_SENTIMENT\n",
       "0                                          FLED ON FOOT              0.0000\n",
       "1                                               neutral              0.0000\n",
       "2      NERVOUS CHANGING DIRECTION FROM OFFICERS RUNNING             -0.2732\n",
       "3                                                  CALM              0.3182\n",
       "4                                                  CALM              0.3182\n",
       "...                                                 ...                 ...\n",
       "15097                                           neutral              0.0000\n",
       "15098                                              CALM              0.3182\n",
       "15099                                         COMPLIANT              0.0000\n",
       "15100                                           ANNOYED             -0.3818\n",
       "15101                             ANNOYED NON COMPLIANT             -0.3818\n",
       "\n",
       "[15102 rows x 2 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
